{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's [AWS SDK for pandas](https://aws-sdk-pandas.readthedocs.io/en/stable/about.html)?\n",
    "\n",
    "An AWS Professional Service open source python initiative that extends the power of the pandas library to AWS, connecting DataFrames and AWS data & analytics services.\n",
    "\n",
    "Easy integration with Athena, Glue, Redshift, Timestream, OpenSearch, Neptune, QuickSight, Chime, CloudWatchLogs, DynamoDB, EMR, SecretManager, PostgreSQL, MySQL, SQLServer and S3 (Parquet, CSV, JSON and EXCEL).\n",
    "\n",
    "Built on top of other open-source projects like Pandas, Apache Arrow and Boto3, it offers abstracted functions to execute your usual ETL tasks like load/unloading data from Data Lakes, Data Warehouses and Databases, even at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "import awswrangler as wr\n",
    "import yfinance as yf\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting datetime variables\n",
    "today = dt.datetime.now()                #Current day of the month\n",
    "start_date = today - timedelta(days=365) #1 year back from the current day\n",
    "end_date = today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Location\": \"http://aws-sdk-pandas72023.s3.amazonaws.com/\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Creating S3 bucket using AWS CLI\n",
    "!aws s3api create-bucket --bucket aws-sdk-pandas72023 --region us-east-2 --create-bucket-configuration LocationConstraint=us-east-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the variable for the bucket name\n",
    "bucket = 'aws-sdk-pandas72023'\n",
    "path = f\"s3://{bucket}/dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS S3 integration\n",
    "\n",
    "##### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_api(ticker:str, start_date:str, end_date:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    tickers(list): List of tickers to be downloaded\n",
    "    start_date(str): Start date of the data\n",
    "    end_date(str): End date of the data\n",
    "    \"\"\"\n",
    "\n",
    "    #Downloading data from yahoo finance api\n",
    "    #If ticker is not available, it will be skipped\n",
    "    #If ticker is available, it will be downloaded and stored in a dataframe\n",
    "    #The dataframe will be returned\n",
    "\n",
    "    try:\n",
    "        ticker = yf.download(tickers=ticker, start=start_date, end=end_date)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return ticker\n",
    "\n",
    "def write_data_to_bucket(file_name:str, mode:str):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    mode(str): Available write modes are 'append', 'overwrite' and 'overwrite_partitions'\n",
    "    \"\"\"\n",
    "\n",
    "    path = f\"s3://{bucket}/raw-data/{file_name}\"\n",
    "    #Sending dataframe of corresponding ticker to bucket\n",
    "    wr.s3.to_csv(\n",
    "        df=df,\n",
    "        path=path,\n",
    "        index=True,\n",
    "        dataset=True,\n",
    "        mode=mode\n",
    "    )\n",
    "\n",
    "def read_csv_from_bucket(folder_name:str) -> pd.DataFrame:\n",
    "\n",
    "    df = wr.s3.read_csv(path = f\"s3://{bucket}/raw-data/{folder_name}/\",\n",
    "                        path_suffix = \".csv\"\n",
    ")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Writing data to S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading a single dataframe from API and loading to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "ticker = 'BTI'\n",
    "\n",
    "#Getting data fro API corresponding to the ticker symbol `British American Tobacco Plc`\n",
    "df = get_data_from_api(ticker,start_date,end_date)\n",
    "\n",
    "#Writing data to the bucket\n",
    "write_data_to_bucket(ticker,'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading a multiple dataframes from API and loading to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "tickers = ['C', 'CAT', 'CL']\n",
    "\n",
    "for ticker in tickers:\n",
    "    #Downloading data from API\n",
    "    df = get_data_from_api(ticker, start_date, end_date)\n",
    "    #Writing data to the bucket\n",
    "    write_data_to_bucket(ticker,'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading data from bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-07-21</td>\n",
       "      <td>178.419998</td>\n",
       "      <td>181.070007</td>\n",
       "      <td>176.479996</td>\n",
       "      <td>180.990005</td>\n",
       "      <td>177.090622</td>\n",
       "      <td>2214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>181.490005</td>\n",
       "      <td>182.800003</td>\n",
       "      <td>177.339996</td>\n",
       "      <td>178.619995</td>\n",
       "      <td>174.771698</td>\n",
       "      <td>1961500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-25</td>\n",
       "      <td>179.169998</td>\n",
       "      <td>182.600006</td>\n",
       "      <td>177.770004</td>\n",
       "      <td>181.809998</td>\n",
       "      <td>177.892975</td>\n",
       "      <td>1845600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-07-26</td>\n",
       "      <td>181.160004</td>\n",
       "      <td>183.649994</td>\n",
       "      <td>180.580002</td>\n",
       "      <td>181.229996</td>\n",
       "      <td>177.325470</td>\n",
       "      <td>1767700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>182.919998</td>\n",
       "      <td>186.179993</td>\n",
       "      <td>180.320007</td>\n",
       "      <td>185.250000</td>\n",
       "      <td>181.258881</td>\n",
       "      <td>1856500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>256.480011</td>\n",
       "      <td>256.480011</td>\n",
       "      <td>252.910004</td>\n",
       "      <td>255.619995</td>\n",
       "      <td>254.360352</td>\n",
       "      <td>1936800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2023-07-17</td>\n",
       "      <td>254.270004</td>\n",
       "      <td>258.850006</td>\n",
       "      <td>252.009995</td>\n",
       "      <td>257.459991</td>\n",
       "      <td>256.191284</td>\n",
       "      <td>2674500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2023-07-18</td>\n",
       "      <td>257.630005</td>\n",
       "      <td>264.160004</td>\n",
       "      <td>256.929993</td>\n",
       "      <td>263.809998</td>\n",
       "      <td>262.509979</td>\n",
       "      <td>3833400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2023-07-19</td>\n",
       "      <td>260.140015</td>\n",
       "      <td>262.920013</td>\n",
       "      <td>259.700012</td>\n",
       "      <td>262.750000</td>\n",
       "      <td>262.750000</td>\n",
       "      <td>2771900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>2023-07-20</td>\n",
       "      <td>264.170013</td>\n",
       "      <td>265.399994</td>\n",
       "      <td>260.440002</td>\n",
       "      <td>261.089996</td>\n",
       "      <td>261.089996</td>\n",
       "      <td>2102300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2022-07-21  178.419998  181.070007  176.479996  180.990005  177.090622   \n",
       "1    2022-07-22  181.490005  182.800003  177.339996  178.619995  174.771698   \n",
       "2    2022-07-25  179.169998  182.600006  177.770004  181.809998  177.892975   \n",
       "3    2022-07-26  181.160004  183.649994  180.580002  181.229996  177.325470   \n",
       "4    2022-07-27  182.919998  186.179993  180.320007  185.250000  181.258881   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "246  2023-07-14  256.480011  256.480011  252.910004  255.619995  254.360352   \n",
       "247  2023-07-17  254.270004  258.850006  252.009995  257.459991  256.191284   \n",
       "248  2023-07-18  257.630005  264.160004  256.929993  263.809998  262.509979   \n",
       "249  2023-07-19  260.140015  262.920013  259.700012  262.750000  262.750000   \n",
       "250  2023-07-20  264.170013  265.399994  260.440002  261.089996  261.089996   \n",
       "\n",
       "      Volume  \n",
       "0    2214500  \n",
       "1    1961500  \n",
       "2    1845600  \n",
       "3    1767700  \n",
       "4    1856500  \n",
       "..       ...  \n",
       "246  1936800  \n",
       "247  2674500  \n",
       "248  3833400  \n",
       "249  2771900  \n",
       "250  2102300  \n",
       "\n",
       "[251 rows x 7 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_csv_from_bucket('CAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.does_object_exist(f's3://{bucket}/raw-data/CAT/3fc6c37a1c344286b83ecb93e83248a2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['TBV']: Exception('%ticker%: No price data found, symbol may be delisted (1d 2022-07-20 21:48:24.632702 -> 2023-07-20 21:48:24.632702)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Open, High, Low, Close, Adj Close, Volume]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_from_api(tickers,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AAPL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m AAPL\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AAPL' is not defined"
     ]
    }
   ],
   "source": [
    "AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://aws-sdk-pandas/dataset/4658d591a5344f48a1a0b73e715873c3.snappy.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"id\": [1, 2],\n",
    "    \"value\": [\"foo\", \"boo\"],\n",
    "    \"date\": [date(2020, 1, 1), date(2020, 1, 2)]\n",
    "})\n",
    "\n",
    "wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path=path,\n",
    "    dataset=True,\n",
    "    mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>value</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>foo</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>boo</td>\n",
       "      <td>2020-01-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id value        date\n",
       "0   1   foo  2020-01-01\n",
       "1   2   boo  2020-01-02"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.read_parquet(path, dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boto3.setup_default_session(region_name=\"us-east-1\")\n",
    "wr.s3.does_object_exist(\"s3://datalake-alphavantage/raw-data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m wr\u001b[39m.\u001b[39;49ms3\u001b[39m.\u001b[39;49mread_parquet(path, dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32md:\\Users\\felix\\anaconda3\\envs\\arrow\\Lib\\site-packages\\awswrangler\\_utils.py:174\u001b[0m, in \u001b[0;36mvalidate_kwargs.<locals>.decorator.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m condition_fn() \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(passed_unsupported_kwargs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    172\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mInvalidArgument(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m}\u001b[39;00m\u001b[39m `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(passed_unsupported_kwargs)\u001b[39m}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Users\\felix\\anaconda3\\envs\\arrow\\Lib\\site-packages\\awswrangler\\_config.py:733\u001b[0m, in \u001b[0;36mapply_configs.<locals>.wrapper\u001b[1;34m(*args_raw, **kwargs)\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[39mdel\u001b[39;00m args[name]\n\u001b[0;32m    732\u001b[0m         args \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkeywords}\n\u001b[1;32m--> 733\u001b[0m \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32md:\\Users\\felix\\anaconda3\\envs\\arrow\\Lib\\site-packages\\awswrangler\\s3\\_read_parquet.py:526\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, path_root, dataset, path_suffix, path_ignore_suffix, ignore_empty, partition_filter, columns, validate_schema, coerce_int96_timestamp_unit, schema, last_modified_begin, last_modified_end, version_id, dtype_backend, chunked, use_threads, ray_args, boto3_session, s3_additional_kwargs, pyarrow_additional_kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39mif\u001b[39;00m chunked:\n\u001b[0;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m _read_parquet_chunked(\n\u001b[0;32m    514\u001b[0m         s3_client\u001b[39m=\u001b[39ms3_client,\n\u001b[0;32m    515\u001b[0m         paths\u001b[39m=\u001b[39mpaths,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         version_ids\u001b[39m=\u001b[39mversion_ids,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 526\u001b[0m \u001b[39mreturn\u001b[39;00m _read_parquet(\n\u001b[0;32m    527\u001b[0m     paths,\n\u001b[0;32m    528\u001b[0m     path_root\u001b[39m=\u001b[39;49mpath_root,\n\u001b[0;32m    529\u001b[0m     schema\u001b[39m=\u001b[39;49mschema,\n\u001b[0;32m    530\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m    531\u001b[0m     coerce_int96_timestamp_unit\u001b[39m=\u001b[39;49mcoerce_int96_timestamp_unit,\n\u001b[0;32m    532\u001b[0m     use_threads\u001b[39m=\u001b[39;49muse_threads,\n\u001b[0;32m    533\u001b[0m     parallelism\u001b[39m=\u001b[39;49mray_args\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mparallelism\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[0;32m    534\u001b[0m     s3_client\u001b[39m=\u001b[39;49ms3_client,\n\u001b[0;32m    535\u001b[0m     s3_additional_kwargs\u001b[39m=\u001b[39;49ms3_additional_kwargs,\n\u001b[0;32m    536\u001b[0m     arrow_kwargs\u001b[39m=\u001b[39;49marrow_kwargs,\n\u001b[0;32m    537\u001b[0m     version_ids\u001b[39m=\u001b[39;49mversion_ids,\n\u001b[0;32m    538\u001b[0m     bulk_read\u001b[39m=\u001b[39;49mbulk_read,\n\u001b[0;32m    539\u001b[0m )\n",
      "File \u001b[1;32md:\\Users\\felix\\anaconda3\\envs\\arrow\\Lib\\site-packages\\awswrangler\\_distributed.py:105\u001b[0m, in \u001b[0;36mEngine.dispatch_on_engine.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Dict[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    104\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39minitialize(name\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget()\u001b[39m.\u001b[39mvalue)\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_func(func)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[1;32md:\\Users\\felix\\anaconda3\\envs\\arrow\\Lib\\site-packages\\awswrangler\\s3\\_read_parquet.py:280\u001b[0m, in \u001b[0;36m_read_parquet\u001b[1;34m(paths, path_root, schema, columns, coerce_int96_timestamp_unit, use_threads, parallelism, version_ids, s3_client, s3_additional_kwargs, arrow_kwargs, bulk_read)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39m@engine\u001b[39m\u001b[39m.\u001b[39mdispatch_on_engine\n\u001b[0;32m    265\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_parquet\u001b[39m(  \u001b[39m# pylint: disable=W0613\u001b[39;00m\n\u001b[0;32m    266\u001b[0m     paths: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    277\u001b[0m     bulk_read: \u001b[39mbool\u001b[39m,\n\u001b[0;32m    278\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[0;32m    279\u001b[0m     executor: _BaseExecutor \u001b[39m=\u001b[39m _get_executor(use_threads\u001b[39m=\u001b[39muse_threads)\n\u001b[1;32m--> 280\u001b[0m     tables \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m    281\u001b[0m         _read_parquet_file,\n\u001b[0;32m    282\u001b[0m         s3_client,\n\u001b[0;32m    283\u001b[0m         paths,\n\u001b[0;32m    284\u001b[0m         itertools\u001b[39m.\u001b[39;49mrepeat(path_root),\n\u001b[0;32m    285\u001b[0m         itertools\u001b[39m.\u001b[39;49mrepeat(columns),\n\u001b[0;32m    286\u001b[0m         itertools\u001b[39m.\u001b[39;49mrepeat(coerce_int96_timestamp_unit),\n\u001b[0;32m    287\u001b[0m         itertools\u001b[39m.\u001b[39;49mrepeat(s3_additional_kwargs),\n\u001b[0;32m    288\u001b[0m         itertools\u001b[39m.\u001b[39;49mrepeat(use_threads),\n\u001b[0;32m    289\u001b[0m         [version_ids\u001b[39m.\u001b[39;49mget(p) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(version_ids, \u001b[39mdict\u001b[39;49m) \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39mfor\u001b[39;49;00m p \u001b[39min\u001b[39;49;00m paths],\n\u001b[0;32m    290\u001b[0m         itertools\u001b[39m.\u001b[39;49mrepeat(schema),\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    292\u001b[0m     \u001b[39mreturn\u001b[39;00m _utils\u001b[39m.\u001b[39mtable_refs_to_df(tables, kwargs\u001b[39m=\u001b[39marrow_kwargs)\n",
      "File \u001b[1;32md:\\Users\\felix\\anaconda3\\envs\\arrow\\Lib\\site-packages\\awswrangler\\_executor.py:51\u001b[0m, in \u001b[0;36m_ThreadPoolExecutor.map\u001b[1;34m(self, func, boto3_client, *args)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     iterables \u001b[39m=\u001b[39m (itertools\u001b[39m.\u001b[39mrepeat(boto3_client), \u001b[39m*\u001b[39margs)\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_exec\u001b[39m.\u001b[39;49mmap(func, \u001b[39m*\u001b[39;49miterables))\n\u001b[0;32m     52\u001b[0m \u001b[39m# Single-threaded\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(func, \u001b[39m*\u001b[39m(itertools\u001b[39m.\u001b[39mrepeat(boto3_client), \u001b[39m*\u001b[39margs)))\n",
      "File \u001b[1;32md:\\Users\\felix\\anaconda3\\envs\\arrow\\Lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39;49mpop())\n\u001b[0;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop(), end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32md:\\Users\\felix\\anaconda3\\envs\\arrow\\Lib\\concurrent\\futures\\_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult(timeout)\n\u001b[0;32m    318\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         fut\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[1;32md:\\Users\\felix\\anaconda3\\envs\\arrow\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32md:\\Users\\felix\\anaconda3\\envs\\arrow\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\felix\\anaconda3\\envs\\arrow\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32md:\\Users\\felix\\anaconda3\\envs\\arrow\\Lib\\site-packages\\awswrangler\\s3\\_read_parquet.py:192\u001b[0m, in \u001b[0;36m_read_parquet_file\u001b[1;34m(s3_client, path, path_root, columns, coerce_int96_timestamp_unit, s3_additional_kwargs, use_threads, version_id, schema)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[39mif\u001b[39;00m schema:\n\u001b[0;32m    188\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    189\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYour version of pyarrow does not support reading with schema. Consider an upgrade to pyarrow 8+.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    190\u001b[0m         \u001b[39mUserWarning\u001b[39;00m,\n\u001b[0;32m    191\u001b[0m     )\n\u001b[1;32m--> 192\u001b[0m pq_file: Optional[pyarrow\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mParquetFile] \u001b[39m=\u001b[39m _pyarrow_parquet_file_wrapper(\n\u001b[0;32m    193\u001b[0m     source\u001b[39m=\u001b[39;49mf,\n\u001b[0;32m    194\u001b[0m     coerce_int96_timestamp_unit\u001b[39m=\u001b[39;49mcoerce_int96_timestamp_unit,\n\u001b[0;32m    195\u001b[0m )\n\u001b[0;32m    196\u001b[0m \u001b[39mif\u001b[39;00m pq_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mInvalidFile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid Parquet file: \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\felix\\anaconda3\\envs\\arrow\\Lib\\site-packages\\awswrangler\\s3\\_read_parquet.py:63\u001b[0m, in \u001b[0;36m_pyarrow_parquet_file_wrapper\u001b[1;34m(source, coerce_int96_timestamp_unit)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pyarrow_parquet_file_wrapper\u001b[39m(\n\u001b[0;32m     59\u001b[0m     source: Any,\n\u001b[0;32m     60\u001b[0m     coerce_int96_timestamp_unit: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     61\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pyarrow\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mParquetFile:\n\u001b[0;32m     62\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[39mreturn\u001b[39;00m pyarrow\u001b[39m.\u001b[39;49mparquet\u001b[39m.\u001b[39;49mParquetFile(source\u001b[39m=\u001b[39;49msource, coerce_int96_timestamp_unit\u001b[39m=\u001b[39;49mcoerce_int96_timestamp_unit)\n\u001b[0;32m     64\u001b[0m     \u001b[39mexcept\u001b[39;00m pyarrow\u001b[39m.\u001b[39mArrowInvalid \u001b[39mas\u001b[39;00m ex:\n\u001b[0;32m     65\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(ex) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mParquet file size is 0 bytes\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\Users\\felix\\anaconda3\\envs\\arrow\\Lib\\site-packages\\pyarrow\\parquet\\core.py:334\u001b[0m, in \u001b[0;36mParquetFile.__init__\u001b[1;34m(self, source, metadata, common_metadata, read_dictionary, memory_map, buffer_size, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, filesystem)\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_source \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# We opened it here, ensure we close it.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader \u001b[39m=\u001b[39m ParquetReader()\n\u001b[1;32m--> 334\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreader\u001b[39m.\u001b[39;49mopen(\n\u001b[0;32m    335\u001b[0m     source, use_memory_map\u001b[39m=\u001b[39;49mmemory_map,\n\u001b[0;32m    336\u001b[0m     buffer_size\u001b[39m=\u001b[39;49mbuffer_size, pre_buffer\u001b[39m=\u001b[39;49mpre_buffer,\n\u001b[0;32m    337\u001b[0m     read_dictionary\u001b[39m=\u001b[39;49mread_dictionary, metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[0;32m    338\u001b[0m     coerce_int96_timestamp_unit\u001b[39m=\u001b[39;49mcoerce_int96_timestamp_unit,\n\u001b[0;32m    339\u001b[0m     decryption_properties\u001b[39m=\u001b[39;49mdecryption_properties,\n\u001b[0;32m    340\u001b[0m     thrift_string_size_limit\u001b[39m=\u001b[39;49mthrift_string_size_limit,\n\u001b[0;32m    341\u001b[0m     thrift_container_size_limit\u001b[39m=\u001b[39;49mthrift_container_size_limit,\n\u001b[0;32m    342\u001b[0m )\n\u001b[0;32m    343\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommon_metadata \u001b[39m=\u001b[39m common_metadata\n\u001b[0;32m    344\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nested_paths_by_prefix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_nested_paths()\n",
      "File \u001b[1;32md:\\Users\\felix\\anaconda3\\envs\\arrow\\Lib\\site-packages\\pyarrow\\_parquet.pyx:1220\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetReader.open\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Users\\felix\\anaconda3\\envs\\arrow\\Lib\\site-packages\\pyarrow\\error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file."
     ]
    }
   ],
   "source": [
    "wr.s3.read_parquet(path, dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
